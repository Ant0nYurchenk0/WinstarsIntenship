{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntr3MYvhSnt9"
      },
      "source": [
        "##Task 2.2\n",
        "\n",
        "*  train NER model for extracting animal titles from the text. Please use some\n",
        "transformer-based model (not LLM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymp_Ck1sTRwT"
      },
      "source": [
        "####Making data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q7nlMIXTUIK"
      },
      "source": [
        "For training a transformer-based model for our purpose we need to have a dataset with tagged words, where there are tags for animals. Unfortunately, I did not manage to find a good dataset online, that would contain not too specific or scientific animal names as well as having words tagged. This is why I decided to generate my own dataset.\n",
        "\n",
        "To generate dataset I combined the LLM approach and sample-sentence approach. The overall process is like so:\n",
        "1.  Our goal is to generate a sentence\n",
        "2.  A sentence consists of 3 parts: beginning, middle section and ending.\n",
        "3.  There are 10 sample sentences of each category: beginning, middle and ending; generated by ChatGPT-4o.\n",
        "4.  Every beginning phrase contains a placeholder for animal name, whereas middle sections and endings might or might not contain such a placeholder.\n",
        "5.  One of each sections are selected at random, merged into one sentence and then a random animal name is put into the placeholder.\n",
        "\n",
        "Because there are 10 different animals and 10 of each type of sentence sections, there are 10000 possible unique sentences. I generate 5000.\n",
        "\n",
        "It is important to highlight, that this approach is not ideal and sentences sometimes make little sence, for example: *Once upon a time, a dog wandered into a mysterious forest. It met a wise old owl who shared a mysterious riddle. At last, the dog found what it had been searching for all along.*\n",
        "\n",
        "The dataset is then saved into the $ner\\_animal\\_generated\\_dataset.csv$ file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HeYZ1G-HcZWy",
        "outputId": "010e07f4-8032-4935-e489-02d34f0eb526"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ner_animal_generated_dataset.csv'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "animal_names = [\"dog\", \"horse\", \"elephant\", \"butterfly\", \"chicken\", \"cat\", \"cow\", \"sheep\", \"spider\", \"squirrel\"]\n",
        "\n",
        "beginnings = [\n",
        "    \"Once upon a time, a {animal} wandered into a mysterious forest.\",\n",
        "    \"In a quiet village, a {animal} discovered an ancient secret.\",\n",
        "    \"A curious {animal} stumbled upon a hidden cave.\",\n",
        "    \"Long ago, a {animal} set off on a grand adventure.\",\n",
        "    \"A lonely {animal} roamed the vast plains in search of something special.\",\n",
        "    \"One evening, a {animal} found itself in an enchanted garden.\",\n",
        "    \"Deep in the jungle, a {animal} heard a strange sound.\",\n",
        "    \"A {animal} in the meadow noticed something glowing in the distance.\",\n",
        "    \"Under the bright moon, a {animal} felt a strange pull towards the river.\",\n",
        "    \"A {animal} in the desert uncovered a long-lost relic.\"\n",
        "]\n",
        "\n",
        "middles = [\n",
        "    \"It met a wise old {animal} who shared a mysterious riddle.\",\n",
        "    \"A sudden storm forced it to seek shelter in a hidden cavern.\",\n",
        "    \"The {animal} found a map leading to a legendary treasure.\",\n",
        "    \"An unexpected friend, a talking bird, guided the {animal} along the way.\",\n",
        "    \"It had to solve a puzzle to continue its journey.\",\n",
        "    \"A mischievous creature tried to trick the {animal} out of its findings.\",\n",
        "    \"The path was blocked by a giant boulder, but a kind {animal} helped move it.\",\n",
        "    \"A magical pond reflected the {animal}'s deepest dreams.\",\n",
        "    \"The {animal} discovered an ancient book filled with forgotten wisdom.\",\n",
        "    \"A hidden passage led the {animal} into a secret underground world.\"\n",
        "]\n",
        "\n",
        "endings = [\n",
        "    \"At last, the {animal} found what it had been searching for all along.\",\n",
        "    \"It returned home, wiser and braver than before.\",\n",
        "    \"The journey changed the {animal} forever, filling its heart with joy.\",\n",
        "    \"A newfound friendship made the adventure truly special.\",\n",
        "    \"The {animal} realized that the real treasure was the memories made.\",\n",
        "    \"With the mystery solved, the {animal} could finally rest.\",\n",
        "    \"The enchanted land bid the {animal} farewell as it continued its journey.\",\n",
        "    \"Having learned an important lesson, the {animal} shared its story with others.\",\n",
        "    \"The {animal} knew it would return one day for another grand adventure.\",\n",
        "    \"As the sun set, the {animal} smiled, knowing its adventure was only the beginning.\"\n",
        "]\n",
        "\n",
        "# Generate unique sentences\n",
        "unique_sentences = set()\n",
        "while len(unique_sentences) < 5000:\n",
        "    animal = random.choice(animal_names)\n",
        "    sentence = f\"{random.choice(beginnings)} {random.choice(middles)} {random.choice(endings)}\".format(animal=animal)\n",
        "\n",
        "    if sentence not in unique_sentences:\n",
        "        unique_sentences.add(sentence)\n",
        "\n",
        "# Restructure the dataset\n",
        "restructured_data = []\n",
        "sentence_id = 1\n",
        "\n",
        "for sentence in unique_sentences:\n",
        "    words = sentence.split()\n",
        "    labels = [\"B-ANIMAL\" if word.lower() in animal_names else \"O\" for word in words]\n",
        "\n",
        "    for word, label in zip(words, labels):\n",
        "        restructured_data.append((sentence_id, word.strip(string.punctuation), label))\n",
        "\n",
        "    sentence_id += 1\n",
        "\n",
        "# Create a DataFrame\n",
        "df_unique_sentences = pd.DataFrame(restructured_data, columns=[\"Sentence Number\", \"Word\", \"Label\"])\n",
        "\n",
        "# Save the dataset to CSV\n",
        "file_path_unique_sentences = \"ner_animal_generated_dataset.csv\"\n",
        "df_unique_sentences.to_csv(file_path_unique_sentences, index=False)\n",
        "\n",
        "# Provide the file to the user\n",
        "file_path_unique_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bv_qgZaVNSX"
      },
      "source": [
        "####Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9ZzfaIzVUIG"
      },
      "source": [
        "For solving the problem I chose to go with the BERT model to classify words in the sentence. This is why I will be using BERT tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TzmL2ss5xVJU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueqw1jNlVjQe"
      },
      "source": [
        "At first we split the dataset into two parts: sentences and tags for words in these sentences. We as well need a label encoder to fit to the tags from the dataset. For this I use the sklearn $preprocessing.LabelEncoder$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FLGVbbKn0A3",
        "outputId": "8e13a5a8-30f2-4afe-8eea-c0d680ed3a69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3149203166.py:3: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[\"Sentence Number\"] = df[\"Sentence Number\"].fillna(method=\"ffill\")\n"
          ]
        }
      ],
      "source": [
        "def process_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
        "    df[\"Sentence Number\"] = df[\"Sentence Number\"].fillna(method=\"ffill\")\n",
        "\n",
        "    enc_label = preprocessing.LabelEncoder()\n",
        "\n",
        "    df[\"Label\"] = enc_label.fit_transform(df[\"Label\"])\n",
        "\n",
        "    sentences = df.groupby(\"Sentence Number\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence Number\")[\"Label\"].apply(list).values\n",
        "    return sentences, tag, enc_label\n",
        "\n",
        "sentence,tag,enc_label = process_data(\"ner_animal_generated_dataset.csv\")\n",
        "animal_id = enc_label.transform([\"B-ANIMAL\"])[0]\n",
        "o_id      = enc_label.transform([\"O\"])[0]\n",
        "pad_id    = max(animal_id, o_id) + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDeSCc6VWG5f"
      },
      "source": [
        "Because models do not work with text data we need to create tokens from words in the sentences. For this I use $BertTokenizerFast$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6jLoR9mfUCq",
        "outputId": "c97223b8-7229-46ef-a12f-db8135132318"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "MAX_LEN = 64\n",
        "\n",
        "def tokenize(sentences, tags, max_len=MAX_LEN, batch_size=32):\n",
        "    all_input_ids = []\n",
        "    all_attention_masks = []\n",
        "    all_labels = []\n",
        "\n",
        "    n = len(sentences)\n",
        "\n",
        "    for start in range(0, n, batch_size):\n",
        "        end = min(start + batch_size, n)\n",
        "        batch_sents = list(sentences[start:end])\n",
        "        batch_tags  = tags[start:end]\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            batch_sents,\n",
        "            is_split_into_words=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        batch_input_ids = encoded[\"input_ids\"]\n",
        "        batch_attention = encoded[\"attention_mask\"]\n",
        "\n",
        "        batch_labels = []\n",
        "        for i, word_labels in enumerate(batch_tags):\n",
        "            word_ids = encoded.word_ids(batch_index=i)\n",
        "\n",
        "            token_labels = []\n",
        "            for wid in word_ids:\n",
        "                if wid is None:\n",
        "                    token_labels.append(pad_id)\n",
        "                else:\n",
        "                    token_labels.append(word_labels[wid])\n",
        "\n",
        "            batch_labels.append(token_labels)\n",
        "\n",
        "        all_input_ids.append(np.array(batch_input_ids))\n",
        "        all_attention_masks.append(np.array(batch_attention))\n",
        "        all_labels.append(np.array(batch_labels))\n",
        "\n",
        "    input_ids = np.concatenate(all_input_ids, axis=0)\n",
        "    attention_masks = np.concatenate(all_attention_masks, axis=0)\n",
        "    labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjiLrFfRWXpH"
      },
      "source": [
        "We then split the data into training and testing datasets and tokenize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNeoonfcqGfr",
        "outputId": "e6ac7170-7874-4b3a-cd95-2366c450ecdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4500,), (500,), (4500,), (500,))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(sentence,tag,random_state=42,test_size=0.1)\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5I3W7Q9bxvWU"
      },
      "outputs": [],
      "source": [
        "input_ids,attention_mask, labels = tokenize(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1yluYvsYxx9l"
      },
      "outputs": [],
      "source": [
        "val_input_ids,val_attention_mask, val_labels = tokenize(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_0rSFu1e-SSK"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "input_ids_t     = torch.tensor(input_ids, dtype=torch.long)\n",
        "attention_mask_t = torch.tensor(attention_mask, dtype=torch.long)\n",
        "labels_t        = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "val_input_ids_t       = torch.tensor(val_input_ids, dtype=torch.long)\n",
        "val_attention_mask_t  = torch.tensor(val_attention_mask, dtype=torch.long)\n",
        "val_labels_t          = torch.tensor(val_labels, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X7rwFH-A-aBd"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(input_ids_t, attention_mask_t, labels_t)\n",
        "val_dataset   = TensorDataset(val_input_ids_t, val_attention_mask_t, val_labels_t)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cULs0aQvWtHu"
      },
      "source": [
        "Because different sentences have different sizes we have data inconsistency. For the model to work we need to pad our sentences. I choose to add padding to make all sentences 64 tokens long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lukbaSPx3um",
        "outputId": "302f82a3-90ce-44be-a055-7ab90f121f49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{64}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TEST: Checking Padding and Truncation length's\n",
        "was = list()\n",
        "for i in range(len(input_ids)):\n",
        "    was.append(len(input_ids[i]))\n",
        "set(was)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1z1Y-2ax7OI",
        "outputId": "3634abcd-dcf4-4ea9-c909-06e38f7d5082"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{64}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train Padding\n",
        "test_tag = list()\n",
        "for i in range(len(y_test)):\n",
        "    test_tag.append(np.array(y_test[i] + [1] * (MAX_LEN-len(y_test[i]))))\n",
        "\n",
        "# TEST:  Checking Padding Length\n",
        "was = list()\n",
        "for i in range(len(test_tag)):\n",
        "    was.append(len(test_tag[i]))\n",
        "set(was)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjpnwsP1x7Qs",
        "outputId": "e5b12006-744d-4fad-b362-2890576b333c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{64}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train Padding\n",
        "train_tag = list()\n",
        "for i in range(len(y_train)):\n",
        "    train_tag.append(np.array(y_train[i] + [1] * (MAX_LEN-len(y_train[i]))))\n",
        "\n",
        "# TEST:  Checking Padding Length\n",
        "was = list()\n",
        "for i in range(len(train_tag)):\n",
        "    was.append(len(train_tag[i]))\n",
        "set(was)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn49xkvqW_De"
      },
      "source": [
        "####Model compilation and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GInBthvIXF5n"
      },
      "source": [
        "Now it comes to designing a model. As mentioned above, I use the $TFBertModel$ to classify tokens as well as also add a couple of additional layers.\n",
        "\n",
        "What is important however is that classes in our dataset are highly impalanced, i.e. there are much more \"other\" words than there are words that are tagged as signing animals. This is why we need to create our own loss function, which highly rewards the correct classification of animal tag to account for class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sroNkl5lyCjx",
        "outputId": "4bbcbc35-6216-46fc-95c1-d35da9484c37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# def custom_loss(y_true, y_pred):\n",
        "#   loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
        "#   mask = tf.cast(tf.not_equal(y_true, pad_id), tf.float32)\n",
        "#   animal_mask = tf.cast(tf.equal(y_true, animal_id), tf.float32)\n",
        "#   weight = 1.0 + 99.0 * animal_mask\n",
        "#   loss = loss * mask * weight\n",
        "#   return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "# def custom_accuracy(y_true, y_pred):\n",
        "#   y_pred_labels = tf.argmax(y_pred, axis=-1, output_type=y_true.dtype)\n",
        "#   mask = tf.cast(tf.not_equal(y_true, pad_id), tf.float32)\n",
        "#   matches = tf.cast(tf.equal(y_true, y_pred_labels), tf.float32)\n",
        "#   matches = matches * mask\n",
        "#   return tf.reduce_sum(matches) / tf.reduce_sum(mask)\n",
        "\n",
        "# def create_model(bert_model,max_len = MAX_LEN):\n",
        "#   input_ids = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n",
        "#   attention_masks = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n",
        "#   bert_output = bert_model(input_ids,attention_mask = attention_masks,return_dict =True)\n",
        "#   embedding = tf.keras.layers.Dropout(0.3)(bert_output[0])\n",
        "#   output = tf.keras.layers.Dense(3,activation = 'softmax')(embedding)\n",
        "#   model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = [output])\n",
        "#   model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-5), loss=custom_loss, metrics=[custom_accuracy])\n",
        "#   return model\n",
        "\n",
        "def custom_accuracy_pytorch(logits, labels, pad_id):\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    mask = labels != pad_id\n",
        "    correct = ((preds == labels) & mask).sum().item()\n",
        "    total = mask.sum().item()\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "    return correct / total\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased',num_labels=3)\n",
        "model.to(device)\n",
        "class_weights = torch.tensor([1.0, 100.0, 0.0], device=device)\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    weight=class_weights,\n",
        "    ignore_index=pad_id,\n",
        ")\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QgDWi8ENyPMn"
      },
      "outputs": [],
      "source": [
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WnL2W48rPkJJ"
      },
      "outputs": [],
      "source": [
        "# history_bert = model.fit([input_ids,attention_mask],np.array(train_tag),validation_data = ([val_input_ids,val_attention_mask],np.array(test_tag)),epochs = 30,batch_size = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eccU2pt8BIRp",
        "outputId": "792b4cdb-bb42-4912-ec96-ee9e705b7b1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train loss=0.0001, val masked acc=1.0000\n",
            "Epoch 2: train loss=0.0001, val masked acc=1.0000\n",
            "Epoch 3: train loss=0.0000, val masked acc=1.0000\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids_b, attention_mask_b, labels_b = [t.to(device) for t in batch]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids_b, attention_mask=attention_mask_b,)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits.view(-1, 3), labels_b.view(-1),)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_acc = 0.0\n",
        "    n_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids_b, attention_mask_b, labels_b = [t.to(device) for t in batch]\n",
        "            outputs = model(input_ids=input_ids_b,attention_mask=attention_mask_b,)\n",
        "            logits = outputs.logits\n",
        "            val_acc += custom_accuracy_pytorch(logits, labels_b, pad_id)\n",
        "            n_batches += 1\n",
        "\n",
        "    val_acc /= max(n_batches, 1)\n",
        "    print(f\"Epoch {epoch+1}: train loss={avg_train_loss:.4f}, val masked acc={val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFWmBblwYmdY"
      },
      "source": [
        "####Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5Xh_CXtYoG9"
      },
      "source": [
        "Here is a bit of code using which we can test our classifier. As you can see, it does not produce correct output. After a bit of research I realized that despite manipulating the dataset and adding a custom-made loss function, the classifier still abuses the rules and assigns \"other\" class to all tokens, and because there are much less animal tokens than all the rest, it gets its score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ-VNhzARGwq"
      },
      "outputs": [],
      "source": [
        "# def pred(val_input_ids,val_attention_mask):\n",
        "#     return model.predict([val_input_ids,val_attention_mask])\n",
        "\n",
        "# def testing(val_input_ids,val_attention_mask,enc_tag,y_test):\n",
        "#     val_input = val_input_ids.reshape(1,MAX_LEN)\n",
        "#     val_attention = val_attention_mask.reshape(1,MAX_LEN)\n",
        "\n",
        "#     # Print Original Sentence\n",
        "#     sentence = tokenizer.decode(val_input_ids[val_input_ids > 0])\n",
        "#     print(\"Original Text : \",str(sentence))\n",
        "#     print(\"\\n\")\n",
        "#     print(y_test)\n",
        "#     true_enc_tag = enc_tag.inverse_transform(y_test)\n",
        "\n",
        "#     print(\"Original Tags : \" ,str(true_enc_tag))\n",
        "#     print(\"\\n\")\n",
        "\n",
        "#     predictions = pred(val_input,val_attention)\n",
        "#     pred_with_pad = np.argmax(predictions,axis = -1)\n",
        "#     pred_without_pad = pred_with_pad[pred_with_pad>0]\n",
        "#     pred_enc_tag = enc_tag.inverse_transform(pred_without_pad)\n",
        "#     print(\"Predicted Tags : \",pred_enc_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1UsCig81CTEY"
      },
      "outputs": [],
      "source": [
        "def pred(val_input_ids, val_attention_mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids_t = torch.tensor(val_input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "        attention_t = torch.tensor(val_attention_mask, dtype=torch.long).unsqueeze(0).to(device)\n",
        "        outputs = model(input_ids=input_ids_t, attention_mask=attention_t)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "    return probs.squeeze(0).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY672hvkCb6y"
      },
      "outputs": [],
      "source": [
        "def testing(val_input_ids, val_attention_mask, enc_tag, y_true_ids):\n",
        "    sentence_ids = val_input_ids[val_input_ids > 0]\n",
        "    sentence = tokenizer.decode(sentence_ids)\n",
        "    print(\"Original Text:\", sentence)\n",
        "    print()\n",
        "\n",
        "    print(\"True label ids:\", y_true_ids)\n",
        "    true_no_pad = y_true_ids[y_true_ids != pad_id]\n",
        "    true_tags = enc_tag.inverse_transform(true_no_pad)\n",
        "    print(\"Original Tags:\", true_tags)\n",
        "    print()\n",
        "\n",
        "    probs = pred(val_input_ids, val_attention_mask)\n",
        "    pred_ids = np.argmax(probs, axis=-1)\n",
        "    pred_no_pad = pred_ids[pred_ids != pad_id]\n",
        "    pred_tags = enc_tag.inverse_transform(pred_no_pad)\n",
        "    print(\"Predicted Tags:\", pred_tags)\n",
        "\n",
        "    animals = []\n",
        "    usable_ids = val_input_ids[val_input_ids > 0]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(usable_ids)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print()\n",
        "    for token, tag in zip(tokens, pred_tags):\n",
        "        if tag == \"B-ANIMAL\":\n",
        "            clean = token.replace(\"##\", \"\")\n",
        "            animals.append(clean)\n",
        "    print(\"Predicted animal words:\", animals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_zFNTerLT_1",
        "outputId": "e7ade120-f0de-4b57-9ca2-9d33b321a336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text: [CLS] long ago a chicken set off on a grand adventure a mischievous fox tried to trick the chicken out of its findings the journey changed the chicken forever filling its heart with joy [SEP]\n",
            "\n",
            "True label ids: [2 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "Original Tags: ['O' 'O' 'O' 'B-ANIMAL' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'B-ANIMAL' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B-ANIMAL' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O']\n",
            "\n",
            "Predicted Tags: ['O' 'O' 'O' 'O' 'B-ANIMAL' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'B-ANIMAL' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B-ANIMAL' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'B-ANIMAL' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
            "Tokens: ['[CLS]', 'long', 'ago', 'a', 'chicken', 'set', 'off', 'on', 'a', 'grand', 'adventure', 'a', 'mischievous', 'fox', 'tried', 'to', 'trick', 'the', 'chicken', 'out', 'of', 'its', 'findings', 'the', 'journey', 'changed', 'the', 'chicken', 'forever', 'filling', 'its', 'heart', 'with', 'joy', '[SEP]']\n",
            "\n",
            "Predicted animal words: ['chicken', 'chicken', 'chicken', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "idx = 25\n",
        "testing(val_input_ids[idx], val_attention_mask[idx], enc_label, val_labels[idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciOtw0NNZ_yW"
      },
      "source": [
        "Now in order to not retrain the model every time we launch the program I save it to the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9Dg_bVZaBPv"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"ner.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY6HUl1QZLDp"
      },
      "source": [
        "####Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCHFWlCSZMZf"
      },
      "source": [
        "In this task I learned to work with NLP, word tokenization and BERT model with Tensorflow. I tried to built a token classifier to recognize names of animals, however could not achive a high enough learning result to actually produce correct results on validation sentences."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
