{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "Ymp_Ck1sTRwT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2.2\n",
        "\n",
        "*  train NER model for extracting animal titles from the text. Please use some\n",
        "transformer-based model (not LLM)."
      ],
      "metadata": {
        "id": "Ntr3MYvhSnt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Making data"
      ],
      "metadata": {
        "id": "Ymp_Ck1sTRwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training a transformer-based model for our purpose we need to have a dataset with tagged words, where there are tags for animals. Unfortunately, I did not manage to find a good dataset online, that would contain not too specific or scientific animal names as well as having words tagged. This is why I decided to generate my own dataset.\n",
        "\n",
        "To generate dataset I combined the LLM approach and sample-sentence approach. The overall process is like so:\n",
        "1.  Our goal is to generate a sentence\n",
        "2.  A sentence consists of 3 parts: beginning, middle section and ending.\n",
        "3.  There are 10 sample sentences of each category: beginning, middle and ending; generated by ChatGPT-4o.\n",
        "4.  Every beginning phrase contains a placeholder for animal name, whereas middle sections and endings might or might not contain such a placeholder.\n",
        "5.  One of each sections are selected at random, merged into one sentence and then a random animal name is put into the placeholder.\n",
        "\n",
        "Because there are 10 different animals and 10 of each type of sentence sections, there are 10000 possible unique sentences. I generate 5000.\n",
        "\n",
        "It is important to highlight, that this approach is not ideal and sentences sometimes make little sence, for example: *Once upon a time, a dog wandered into a mysterious forest. It met a wise old owl who shared a mysterious riddle. At last, the dog found what it had been searching for all along.*\n",
        "\n",
        "The dataset is then saved into the $ner\\_animal\\_generated\\_dataset.csv$ file."
      ],
      "metadata": {
        "id": "8Q7nlMIXTUIK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HeYZ1G-HcZWy",
        "outputId": "a548e817-03c5-4347-d5fd-5fb13b12d388"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ner_animal_generated_dataset.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "animal_names = [\"dog\", \"horse\", \"elephant\", \"butterfly\", \"chicken\", \"cat\", \"cow\", \"sheep\", \"spider\", \"squirrel\"]\n",
        "\n",
        "beginnings = [\n",
        "    \"Once upon a time, a {animal} wandered into a mysterious forest.\",\n",
        "    \"In a quiet village, a {animal} discovered an ancient secret.\",\n",
        "    \"A curious {animal} stumbled upon a hidden cave.\",\n",
        "    \"Long ago, a {animal} set off on a grand adventure.\",\n",
        "    \"A lonely {animal} roamed the vast plains in search of something special.\",\n",
        "    \"One evening, a {animal} found itself in an enchanted garden.\",\n",
        "    \"Deep in the jungle, a {animal} heard a strange sound.\",\n",
        "    \"A {animal} in the meadow noticed something glowing in the distance.\",\n",
        "    \"Under the bright moon, a {animal} felt a strange pull towards the river.\",\n",
        "    \"A {animal} in the desert uncovered a long-lost relic.\"\n",
        "]\n",
        "\n",
        "middles = [\n",
        "    \"It met a wise old owl who shared a mysterious riddle.\",\n",
        "    \"A sudden storm forced it to seek shelter in a hidden cavern.\",\n",
        "    \"The {animal} found a map leading to a legendary treasure.\",\n",
        "    \"An unexpected friend, a talking parrot, guided the {animal} along the way.\",\n",
        "    \"It had to solve a puzzle to continue its journey.\",\n",
        "    \"A mischievous fox tried to trick the {animal} out of its findings.\",\n",
        "    \"The path was blocked by a giant boulder, but a kind bear helped move it.\",\n",
        "    \"A magical pond reflected the {animal}'s deepest dreams.\",\n",
        "    \"The {animal} discovered an ancient book filled with forgotten wisdom.\",\n",
        "    \"A hidden passage led the {animal} into a secret underground world.\"\n",
        "]\n",
        "\n",
        "endings = [\n",
        "    \"At last, the {animal} found what it had been searching for all along.\",\n",
        "    \"It returned home, wiser and braver than before.\",\n",
        "    \"The journey changed the {animal} forever, filling its heart with joy.\",\n",
        "    \"A newfound friendship made the adventure truly special.\",\n",
        "    \"The {animal} realized that the real treasure was the memories made.\",\n",
        "    \"With the mystery solved, the {animal} could finally rest.\",\n",
        "    \"The enchanted land bid the {animal} farewell as it continued its journey.\",\n",
        "    \"Having learned an important lesson, the {animal} shared its story with others.\",\n",
        "    \"The {animal} knew it would return one day for another grand adventure.\",\n",
        "    \"As the sun set, the {animal} smiled, knowing its adventure was only the beginning.\"\n",
        "]\n",
        "\n",
        "# Generate unique sentences\n",
        "unique_sentences = set()\n",
        "while len(unique_sentences) < 5000:\n",
        "    animal = random.choice(animal_names)\n",
        "    sentence = f\"{random.choice(beginnings)} {random.choice(middles)} {random.choice(endings)}\".format(animal=animal)\n",
        "\n",
        "    if sentence not in unique_sentences:\n",
        "        unique_sentences.add(sentence)\n",
        "\n",
        "# Restructure the dataset\n",
        "restructured_data = []\n",
        "sentence_id = 1\n",
        "\n",
        "for sentence in unique_sentences:\n",
        "    words = sentence.split()\n",
        "    labels = [\"B-ANIMAL\" if word.lower() in animal_names else \"O\" for word in words]\n",
        "\n",
        "    for word, label in zip(words, labels):\n",
        "        restructured_data.append((sentence_id, word.strip(string.punctuation), label))\n",
        "\n",
        "    sentence_id += 1\n",
        "\n",
        "# Create a DataFrame\n",
        "df_unique_sentences = pd.DataFrame(restructured_data, columns=[\"Sentence Number\", \"Word\", \"Label\"])\n",
        "\n",
        "# Save the dataset to CSV\n",
        "file_path_unique_sentences = \"ner_animal_generated_dataset.csv\"\n",
        "df_unique_sentences.to_csv(file_path_unique_sentences, index=False)\n",
        "\n",
        "# Provide the file to the user\n",
        "file_path_unique_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data preprocessing"
      ],
      "metadata": {
        "id": "3bv_qgZaVNSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For solving the problem I chose to go with the BERT model to classify words in the sentence. This is why I will be using BERT tokenizer."
      ],
      "metadata": {
        "id": "r9ZzfaIzVUIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertModel, logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "TzmL2ss5xVJU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first we split the dataset into two parts: sentences and tags for words in these sentences. We as well need a label encoder to fit to the tags from the dataset. For this I use the sklearn $preprocessing.LabelEncoder$."
      ],
      "metadata": {
        "id": "ueqw1jNlVjQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
        "    df.loc[:, \"Sentence Number\"] = df[\"Sentence Number\"].fillna(method=\"ffill\")\n",
        "\n",
        "    enc_label = preprocessing.LabelEncoder()\n",
        "\n",
        "    df.loc[:, \"Label\"] = enc_label.fit_transform(df[\"Label\"])\n",
        "\n",
        "    sentences = df.groupby(\"Sentence Number\")[\"Word\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence Number\")[\"Label\"].apply(list).values\n",
        "    return sentences, tag, enc_label\n",
        "\n",
        "sentence,tag,enc_label = process_data(\"ner_animal_generated_dataset.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FLGVbbKn0A3",
        "outputId": "e2497ab0-121f-4e1a-b8ec-7c7ed53f8afa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e54703f46e6b>:3: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df.loc[:, \"Sentence Number\"] = df[\"Sentence Number\"].fillna(method=\"ffill\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because models do not work with text data we need to create tokens from words in the sentences. For this I use $BertTokenizerFast$."
      ],
      "metadata": {
        "id": "uDeSCc6VWG5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "MAX_LEN = 64\n",
        "def tokenize(data,max_len = MAX_LEN):\n",
        "    input_ids = list()\n",
        "    attention_mask = list()\n",
        "    for i in tqdm(range(len(data))):\n",
        "        encoded = tokenizer.encode_plus(data[i],\n",
        "                                        add_special_tokens = True,\n",
        "                                        max_length = MAX_LEN,\n",
        "                                        is_split_into_words=True,\n",
        "                                        return_attention_mask=True,\n",
        "                                        padding = 'max_length',\n",
        "                                        truncation=True,return_tensors = 'np')\n",
        "\n",
        "\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_mask.append(encoded['attention_mask'])\n",
        "    return np.vstack(input_ids),np.vstack(attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6jLoR9mfUCq",
        "outputId": "d74389e0-be6d-49bc-861a-367219a79f22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then split the data into training and testing datasets and tokenize them."
      ],
      "metadata": {
        "id": "ZjiLrFfRWXpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(sentence,tag,random_state=42,test_size=0.1)\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNeoonfcqGfr",
        "outputId": "10f603a8-b6e9-4dc2-9f64-fa6afcd43856"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4500,), (500,), (4500,), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids,attention_mask = tokenize(X_train,max_len = MAX_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I3W7Q9bxvWU",
        "outputId": "a8f2e218-229b-4992-8544-20043e94a645"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4500/4500 [00:01<00:00, 3035.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_ids,val_attention_mask = tokenize(X_test,max_len = MAX_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yluYvsYxx9l",
        "outputId": "75997b94-9a26-40ee-fbc4-c9b323a25efc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 3066.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because different sentences have different sizes we have data inconsistency. For the model to work we need to pad our sentences. I choose to add padding to make all sentences 64 tokens long."
      ],
      "metadata": {
        "id": "cULs0aQvWtHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Checking Padding and Truncation length's\n",
        "was = list()\n",
        "for i in range(len(input_ids)):\n",
        "    was.append(len(input_ids[i]))\n",
        "set(was)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lukbaSPx3um",
        "outputId": "4b25b369-3bf0-4c43-93c5-f936d2ab3a15"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{64}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Padding\n",
        "test_tag = list()\n",
        "for i in range(len(y_test)):\n",
        "    test_tag.append(np.array(y_test[i] + [1] * (MAX_LEN-len(y_test[i]))))\n",
        "\n",
        "# TEST:  Checking Padding Length\n",
        "was = list()\n",
        "for i in range(len(test_tag)):\n",
        "    was.append(len(test_tag[i]))\n",
        "set(was)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1z1Y-2ax7OI",
        "outputId": "65a22e4e-62a0-491d-ce88-9f5beabeddf9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{64}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Padding\n",
        "train_tag = list()\n",
        "for i in range(len(y_train)):\n",
        "    train_tag.append(np.array(y_train[i] + [1] * (MAX_LEN-len(y_train[i]))))\n",
        "\n",
        "# TEST:  Checking Padding Length\n",
        "was = list()\n",
        "for i in range(len(train_tag)):\n",
        "    was.append(len(train_tag[i]))\n",
        "set(was)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjpnwsP1x7Qs",
        "outputId": "0d7e2259-5fe5-4cd7-c63f-1a0c9b933d4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{64}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Model compilation and training"
      ],
      "metadata": {
        "id": "Mn49xkvqW_De"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it comes to designing a model. As mentioned above, I use the $TFBertModel$ to classify tokens as well as also add a couple of additional layers.\n",
        "\n",
        "What is important however is that classes in our dataset are highly impalanced, i.e. there are much more \"other\" words than there are words that are tagged as signing animals. This is why we need to create our own loss function, which highly rewards the correct classification of animal tag to account for class imbalance."
      ],
      "metadata": {
        "id": "GInBthvIXF5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "  # Compute the sparse categorical crossentropy loss per token.\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "  # For tokens where y_true is 0 (\"B-ANIMAL\"), multiply the loss by a factor (e.g. 100)\n",
        "  animal_weight = tf.where(tf.equal(y_true, 0), 100.0, 1.0)\n",
        "  weighted_loss = loss * animal_weight\n",
        "  return tf.reduce_mean(weighted_loss)\n",
        "\n",
        "def create_model(bert_model,max_len = MAX_LEN):\n",
        "  input_ids = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n",
        "  attention_masks = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n",
        "  bert_output = bert_model(input_ids,attention_mask = attention_masks,return_dict =True)\n",
        "  embedding = tf.keras.layers.Dropout(0.3)(bert_output[0])\n",
        "  output = tf.keras.layers.Dense(2,activation = 'softmax')(embedding)\n",
        "  model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = [output])\n",
        "  model.compile(optimizer=tf.keras.optimizers.AdamW(lr=0.00001), loss=custom_loss, metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "bert_model = TFBertModel.from_pretrained(pretrained_model_name_or_path='bert-base-uncased', num_labels=2)\n",
        "model = create_model(bert_model, MAX_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sroNkl5lyCjx",
        "outputId": "f518c967-9796-4195-a29b-b491f659f623"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.AdamW.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "QgDWi8ENyPMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31b6b70-f483-479e-ca49-78276857786e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1094822   ['input_1[0][0]',             \n",
            " )                           ngAndCrossAttentions(last_   40         'input_2[0][0]']             \n",
            "                             hidden_state=(None, 64, 76                                           \n",
            "                             8),                                                                  \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)        (None, 64, 768)              0         ['tf_bert_model[0][0]']       \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 64, 2)                1538      ['dropout_37[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109483778 (417.65 MB)\n",
            "Trainable params: 109483778 (417.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_bert = model.fit([input_ids,attention_mask],np.array(train_tag),validation_data = ([val_input_ids,val_attention_mask],np.array(test_tag)),epochs = 30,batch_size = 32)"
      ],
      "metadata": {
        "id": "WnL2W48rPkJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c7b9ea0-abeb-4fa0-a52e-e3a02eba50fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141/141 [==============================] - 57s 106ms/step - loss: 3.5323 - accuracy: 0.0720 - val_loss: 2.3643 - val_accuracy: 0.0360\n",
            "Epoch 2/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3934 - accuracy: 0.0360 - val_loss: 2.3532 - val_accuracy: 0.0360\n",
            "Epoch 3/30\n",
            "141/141 [==============================] - 10s 71ms/step - loss: 2.3727 - accuracy: 0.0360 - val_loss: 2.3548 - val_accuracy: 0.0360\n",
            "Epoch 4/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3708 - accuracy: 0.0360 - val_loss: 2.3532 - val_accuracy: 0.0360\n",
            "Epoch 5/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3631 - accuracy: 0.0360 - val_loss: 2.3611 - val_accuracy: 0.0360\n",
            "Epoch 6/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3671 - accuracy: 0.0360 - val_loss: 2.3531 - val_accuracy: 0.0360\n",
            "Epoch 7/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3605 - accuracy: 0.0360 - val_loss: 2.3544 - val_accuracy: 0.0360\n",
            "Epoch 8/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3633 - accuracy: 0.0360 - val_loss: 2.3537 - val_accuracy: 0.0360\n",
            "Epoch 9/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3618 - accuracy: 0.0360 - val_loss: 2.3532 - val_accuracy: 0.0360\n",
            "Epoch 10/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3613 - accuracy: 0.0360 - val_loss: 2.3534 - val_accuracy: 0.0360\n",
            "Epoch 11/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3592 - accuracy: 0.0360 - val_loss: 2.3537 - val_accuracy: 0.0360\n",
            "Epoch 12/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3605 - accuracy: 0.0360 - val_loss: 2.3552 - val_accuracy: 0.0360\n",
            "Epoch 13/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3594 - accuracy: 0.0360 - val_loss: 2.3545 - val_accuracy: 0.0360\n",
            "Epoch 14/30\n",
            "141/141 [==============================] - 10s 70ms/step - loss: 2.3596 - accuracy: 0.0360 - val_loss: 2.3564 - val_accuracy: 0.0360\n",
            "Epoch 15/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3568 - accuracy: 0.0360 - val_loss: 2.3557 - val_accuracy: 0.0360\n",
            "Epoch 16/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3600 - accuracy: 0.0360 - val_loss: 2.3536 - val_accuracy: 0.0360\n",
            "Epoch 17/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3591 - accuracy: 0.0360 - val_loss: 2.3553 - val_accuracy: 0.0360\n",
            "Epoch 18/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3586 - accuracy: 0.0360 - val_loss: 2.3553 - val_accuracy: 0.0360\n",
            "Epoch 19/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3584 - accuracy: 0.0360 - val_loss: 2.3531 - val_accuracy: 0.0360\n",
            "Epoch 20/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3581 - accuracy: 0.0360 - val_loss: 2.3531 - val_accuracy: 0.0360\n",
            "Epoch 21/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3582 - accuracy: 0.0360 - val_loss: 2.3562 - val_accuracy: 0.0360\n",
            "Epoch 22/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3593 - accuracy: 0.0360 - val_loss: 2.3598 - val_accuracy: 0.0360\n",
            "Epoch 23/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3584 - accuracy: 0.0360 - val_loss: 2.3533 - val_accuracy: 0.0360\n",
            "Epoch 24/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3578 - accuracy: 0.0360 - val_loss: 2.3562 - val_accuracy: 0.0360\n",
            "Epoch 25/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3577 - accuracy: 0.0360 - val_loss: 2.3544 - val_accuracy: 0.0360\n",
            "Epoch 26/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3557 - accuracy: 0.0360 - val_loss: 2.3545 - val_accuracy: 0.0360\n",
            "Epoch 27/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3568 - accuracy: 0.0360 - val_loss: 2.3530 - val_accuracy: 0.0360\n",
            "Epoch 28/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3573 - accuracy: 0.0360 - val_loss: 2.3575 - val_accuracy: 0.0360\n",
            "Epoch 29/30\n",
            "141/141 [==============================] - 10s 68ms/step - loss: 2.3564 - accuracy: 0.0360 - val_loss: 2.3530 - val_accuracy: 0.0360\n",
            "Epoch 30/30\n",
            "141/141 [==============================] - 10s 69ms/step - loss: 2.3562 - accuracy: 0.0360 - val_loss: 2.3532 - val_accuracy: 0.0360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Results"
      ],
      "metadata": {
        "id": "NFWmBblwYmdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a bit of code using which we can test our classifier. As you can see, it does not produce correct output. After a bit of research I realized that despite manipulating the dataset and adding a custom-made loss function, the classifier still abuses the rules and assigns \"other\" class to all tokens, and because there are much less animal tokens than all the rest, it gets its score.\n",
        "\n",
        "Unfortunately I was not able to beat this issue."
      ],
      "metadata": {
        "id": "r5Xh_CXtYoG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(val_input_ids,val_attention_mask):\n",
        "    return model.predict([val_input_ids,val_attention_mask])\n",
        "\n",
        "def testing(val_input_ids,val_attention_mask,enc_tag,y_test):\n",
        "    val_input = val_input_ids.reshape(1,MAX_LEN)\n",
        "    val_attention = val_attention_mask.reshape(1,MAX_LEN)\n",
        "\n",
        "    # Print Original Sentence\n",
        "    sentence = tokenizer.decode(val_input_ids[val_input_ids > 0])\n",
        "    print(\"Original Text : \",str(sentence))\n",
        "    print(\"\\n\")\n",
        "    print(y_test)\n",
        "    true_enc_tag = enc_tag.inverse_transform(y_test)\n",
        "\n",
        "    print(\"Original Tags : \" ,str(true_enc_tag))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    predictions = pred(val_input,val_attention)\n",
        "    pred_with_pad = np.argmax(predictions,axis = -1)\n",
        "    pred_without_pad = pred_with_pad[pred_with_pad>0]\n",
        "    pred_enc_tag = enc_tag.inverse_transform(pred_without_pad)\n",
        "    print(\"Predicted Tags : \",pred_enc_tag)"
      ],
      "metadata": {
        "id": "qJ-VNhzARGwq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing(val_input_ids[0],val_attention_mask[0],enc_label,y_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_zFNTerLT_1",
        "outputId": "9bf8fc4e-f200-4108-ad66-87462b87e0dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text :  [CLS] one evening a horse found itself in an enchanted garden the horse discovered an ancient book filled with forgotten wisdom the horse knew it would return one day for another grand adventure [SEP]\n",
            "\n",
            "\n",
            "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Original Tags :  ['O' 'O' 'O' 'B-ANIMAL' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B-ANIMAL' 'O' 'O' 'O'\n",
            " 'O' 'O' 'O' 'O' 'O' 'O' 'B-ANIMAL' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
            " 'O']\n",
            "\n",
            "\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "Predicted Tags :  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now in order to not retrain the model every time we launch the program I save it to the file."
      ],
      "metadata": {
        "id": "ciOtw0NNZ_yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path='ner.h5'\n",
        "model.save(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Dg_bVZaBPv",
        "outputId": "aef9bde0-28b2-49c3-a445-c6716087925f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Conclusions"
      ],
      "metadata": {
        "id": "jY6HUl1QZLDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task I learned to work with NLP, word tokenization and BERT model with Tensorflow. I tried to built a token classifier to recognize names of animals, however could not achive a high enough learning result to actually produce correct results on validation sentences."
      ],
      "metadata": {
        "id": "OCHFWlCSZMZf"
      }
    }
  ]
}