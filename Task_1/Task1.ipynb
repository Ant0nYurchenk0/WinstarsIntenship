{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Task 1. Image classification + OOP\n",
        "In this task, you need to use a publicly available simple MNIST dataset and build 3 classification\n",
        "models around it. It should be the following models:\n",
        "\n",
        "\n",
        "1.   Random Forest;\n",
        "2.   Feed-Forward Neural Network;\n",
        "3.   Convolutional Neural Network;\n",
        "\n",
        "Each model should be a separate class that implements $MnistClassifierInterface$ with 2\n",
        "abstract methods - train and predict. Finally, each of your three models should be hidden under\n",
        "another $MnistClassifier$ class. $MnistClassifer$ takes an algorithm as an input parameter.\n",
        "Possible values for the algorithm are: $cnn$, $rf$, and $nn$ for the three models described above.\n",
        "The solution should contain:\n",
        "*   Interface for models called $MnistClassifierInterface$.\n",
        "*   3 classes (1 for each model) that implement $MnistClassifierInterface$.\n",
        "*   $MnistClassifier$, which takes as an input parameter the name of the algorithm and\n",
        "provides predictions with exactly the same structure (inputs and outputs) not depending\n",
        "on the selected algorithm."
      ],
      "metadata": {
        "id": "4PCbkqts1bmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data"
      ],
      "metadata": {
        "id": "Q61E4e_D8OX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all let's import the data. For this I use the Tensorflow library that has a built-in API for the MNIST dataset.\n",
        "\n",
        "The data in the mnist dataset is represented as matrix of 28 × 28 grayscale pixels that have values from 0 to 255. First of all we need to normalize data, since it enhances training stability and convergence speed but also improves the generalization and accuracy of CNN models.\n",
        "\n",
        "Then we also need to split data into the training and testing dataset. I use a standard ratio of 4:1 i.e. 80% of data is used for training and 20% for testing."
      ],
      "metadata": {
        "id": "RcCTwZGI19YD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEWk79LPFI4f",
        "outputId": "bb5d85c2-4c23-43e3-d390-3fd13618ba4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images are of shape: (60000, 28, 28) and labels: (60000,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "SPLIT_PERCENTAGE = 0.8\n",
        "\n",
        "\n",
        "(images, labels), (_, _) = mnist.load_data()\n",
        "images = images / 255  # normalising images\n",
        "\n",
        "print(f\"images are of shape: {images.shape} and labels: {labels.shape}\")\n",
        "\n",
        "size = images.shape[0]\n",
        "split = int(size * SPLIT_PERCENTAGE)\n",
        "# Subsample the images\n",
        "train_images = images[:split]\n",
        "train_labels = labels[:split]\n",
        "\n",
        "test_images = images[split:]\n",
        "test_labels = labels[split:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Classifiers"
      ],
      "metadata": {
        "id": "DVSKgT9B8R3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's step back and create interface class. For this I use the abcplus library. In this class $MnistClassifierInterface$ I create three abstract methods: $train$ and $predict$ from the conditions of the problem and also $evaluate\\_accuracy$ to later compare how different models perform on the same data.\n",
        "\n",
        "Methods do not contain any implementation. However, I add an implementation of constructor to list all the common protected variables I will be using in the subclasses."
      ],
      "metadata": {
        "id": "opVNUbZC3JzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class MnistClassifierInterface(ABC):\n",
        "    def __init__(self):\n",
        "        self._train_images = np.ndarray(0)\n",
        "        self._train_labels = np.ndarray(0)\n",
        "        self._test_images = np.ndarray(0)\n",
        "        self._test_labels = np.ndarray(0)\n",
        "        self._predictions = np.ndarray(0)\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self, train_images: np.ndarray, train_labels: np.ndarray) -> None:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, test_images: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def evaluate_accuracy(self, test_labels: np.ndarray) -> float:\n",
        "        pass"
      ],
      "metadata": {
        "id": "BYTa65xdFfg_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Random Forest classifier I use the sklearn library with its $RandomForestClassifier$ implementation.\n",
        "\n",
        "However, to work correctly this class needs some additional preprocessing of data. As we originally work with images, they are represented as 2D arrays. But sklearn's classifiers work with 1D arrays only. So we flatten each image into a vector.\n",
        "\n",
        "For measuring the goodness of performance I use the $accuracy\\_score$ from sklearn metrics."
      ],
      "metadata": {
        "id": "QPcVtUBY4GGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "class RFClassifier(MnistClassifierInterface):\n",
        "    def __init__(self):\n",
        "        self._predictor = RandomForestClassifier()\n",
        "        super()\n",
        "\n",
        "    def train(self, train_images: np.ndarray, train_labels: np.ndarray) -> None:\n",
        "        self._train_images = [image.flatten() for image in train_images]\n",
        "        self._train_labels = train_labels\n",
        "        self._predictor.fit(self._train_images, self._train_labels)\n",
        "\n",
        "    def predict(self, test_images: np.ndarray) -> np.ndarray:\n",
        "        self._test_images = [image.flatten() for image in test_images]\n",
        "        self._predictions = self._predictor.predict(self._test_images)\n",
        "        return self._predictions\n",
        "\n",
        "    def evaluate_accuracy(self, test_labels: np.ndarray) -> float:\n",
        "        self._test_labels = test_labels\n",
        "        return accuracy_score(self._test_labels, self._predictions)\n"
      ],
      "metadata": {
        "id": "BG1wqC9gGyK6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it comes to neural networks I switch to the Tensorflow library.\n",
        "\n",
        "For the feed-forward neural network I create a model with only 3 layers. First one takes the input data and flattens it similar to what I manually did in the case with Random Forest. After that there are two fully connected layers, the latter of which has only 10 nodes which corresponds to the number of classes in the dataset (which contains handwritten digits 0-9).\n",
        "\n",
        "For measurring accuracy score I use the built-in Tensorflow function $evaluate$, that evaluates the performance of the model. This function also does return the loss value, however I disregard it in this task."
      ],
      "metadata": {
        "id": "CVTH-g635Af3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "\n",
        "\n",
        "class FFNNClassifier(MnistClassifierInterface):\n",
        "    def __init__(self):\n",
        "        self.epochs = 5\n",
        "        self._predictor = Sequential(\n",
        "            [\n",
        "                Flatten(input_shape=(28, 28)),\n",
        "                Dense(128, activation=\"relu\"),\n",
        "                Dense(10, activation=\"softmax\"),\n",
        "            ]\n",
        "        )\n",
        "        self._predictor.compile(\n",
        "            optimizer=Adam(),\n",
        "            loss=SparseCategoricalCrossentropy(),\n",
        "            metrics=[SparseCategoricalAccuracy()],\n",
        "        )\n",
        "        super()\n",
        "\n",
        "    def train(self, train_images: np.ndarray, train_labels: np.ndarray) -> None:\n",
        "        self._train_images = train_images\n",
        "        self._train_labels = train_labels\n",
        "        self._predictor.fit(\n",
        "            self._train_images, self._train_labels, epochs=self.epochs, verbose=0\n",
        "        )\n",
        "\n",
        "    def predict(self, test_images: np.ndarray) -> np.ndarray:\n",
        "        self._test_images = test_images\n",
        "        return self._predictor.predict(self._test_images)\n",
        "\n",
        "    def evaluate_accuracy(self, test_labels: np.ndarray) -> float:\n",
        "        self._test_labels = test_labels\n",
        "        _, test_acc = self._predictor.evaluate(self._test_images, self._test_labels)\n",
        "        return test_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "kAsYIJgoLx1U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In convolutional neural network classifier I deviate slightly from the initial task and make the $CNNClassifier$ a subclass of $FFNNClassifier$, which in turn implements the $MnistClassifierInterface$, but not a direct implementation of $MnistClassifierInterface$. I do this, because the implementation of all the three methods is identical to feed-forward classifier, and the only thing that is different is the constructor, where we set up the architecture of the model.\n",
        "\n",
        "For the model itself, I, again, use the same approach as previously, however, I add two pairs of alternating convolutional and pooling layers."
      ],
      "metadata": {
        "id": "qSeV7VJR6IBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "class CNNClassifier(FFNNClassifier):\n",
        "    def __init__(self):\n",
        "        self.epochs = 5\n",
        "        self._predictor = Sequential(\n",
        "            [\n",
        "                Conv2D(64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "                MaxPooling2D(2, 2),\n",
        "                Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "                MaxPooling2D(2, 2),\n",
        "                Flatten(),\n",
        "                Dense(128, activation=\"relu\"),\n",
        "                Dense(10, activation=\"softmax\"),\n",
        "            ]\n",
        "        )\n",
        "        self._predictor.compile(\n",
        "            optimizer=Adam(),\n",
        "            loss=SparseCategoricalCrossentropy(),\n",
        "            metrics=[SparseCategoricalAccuracy()],\n",
        "        )\n",
        "        super()\n"
      ],
      "metadata": {
        "id": "T_-AfC53PdZ0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, for the wrapper $MnistClassifier$ class I add a constructor parameter for choosing an algorithm and the $classify$ method that launches the training, prediction and performance evaluation phases of the chosen algorithm.\n",
        "\n",
        "As a result, I return the predictions that the algorithm made based on the testing data as well as the accuracy score of the model."
      ],
      "metadata": {
        "id": "8OPhS-qx7LvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "\n",
        "class MnistClassifier:\n",
        "    def __init__(self, algorithm: str):\n",
        "        match (algorithm):\n",
        "            case \"rf\":\n",
        "                self._classifier = RFClassifier()\n",
        "            case \"nn\":\n",
        "                self._classifier = FFNNClassifier()\n",
        "            case \"cnn\":\n",
        "                self._classifier = CNNClassifier()\n",
        "            case _:\n",
        "                raise ValueError(\"Invalid algorithm\")\n",
        "        self._predictions = np.ndarray(0)\n",
        "        self._accuracy = 0\n",
        "\n",
        "    def classify(\n",
        "        self,\n",
        "        train_images: np.ndarray,\n",
        "        train_labels: np.ndarray,\n",
        "        test_images: np.ndarray,\n",
        "        test_labels: np.ndarray,\n",
        "    ) -> Tuple[np.ndarray, float]:\n",
        "        self._classifier.train(train_images, train_labels)\n",
        "        self._predictions = self._classifier.predict(test_images)\n",
        "        self._accuracy = self._classifier.evaluate_accuracy(test_labels)\n",
        "        return (self._predictions, self._accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "T-lX3qGuX8kk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Results"
      ],
      "metadata": {
        "id": "1j1-Sym-8U4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the visualization of the work of the three algorithms I create three sepparate objects and launch the respective classifier in each of them. After that I print the accuracy score of each model in color for better contrast.\n",
        "\n",
        "The predictions array is disregarded as there is no use of it in this case."
      ],
      "metadata": {
        "id": "kkxGtQox7tio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class bcolors:\n",
        "    HEADER = \"\\033[95m\"\n",
        "    OKBLUE = \"\\033[94m\"\n",
        "    OKCYAN = \"\\033[96m\"\n",
        "    OKGREEN = \"\\033[92m\"\n",
        "    WARNING = \"\\033[93m\"\n",
        "    FAIL = \"\\033[91m\"\n",
        "    ENDC = \"\\033[0m\"\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNDERLINE = \"\\033[4m\""
      ],
      "metadata": {
        "id": "Fu7sSYF5uPD5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RFClassifier = MnistClassifier(\"rf\")\n",
        "_, RFAcc = RFClassifier.classify(\n",
        "    train_images, train_labels, test_images, test_labels\n",
        ")\n",
        "print(\n",
        "    bcolors.OKGREEN + \"Random forest accuracy score: \" + str(RFAcc) + bcolors.ENDC\n",
        ")\n",
        "\n",
        "FFNNClassifier = MnistClassifier(\"nn\")\n",
        "_, FFNNAcc = FFNNClassifier.classify(\n",
        "    train_images, train_labels, test_images, test_labels\n",
        ")\n",
        "print(\n",
        "    bcolors.OKGREEN\n",
        "    + \"Fast-forward neural network accuracy score: \"\n",
        "    + str(FFNNAcc)\n",
        "    + bcolors.ENDC\n",
        ")\n",
        "\n",
        "CNNClassifier = MnistClassifier(\"cnn\")\n",
        "_, CNNAcc = CNNClassifier.classify(\n",
        "    train_images, train_labels, test_images, test_labels\n",
        ")\n",
        "print(\n",
        "    bcolors.OKGREEN\n",
        "    + \"Convolutional neural network accuracy score: \"\n",
        "    + str(CNNAcc)\n",
        "    + bcolors.ENDC\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXY90eYA0aQ1",
        "outputId": "6a925c12-6369-4c0a-d955-175c089ecaf9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mRandom forest accuracy score: 0.9705833333333334\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0936 - sparse_categorical_accuracy: 0.9696\n",
            "\u001b[92mFast-forward neural network accuracy score: 0.9731666445732117\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9880\n",
            "\u001b[92mConvolutional neural network accuracy score: 0.9881666898727417\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Conclusion"
      ],
      "metadata": {
        "id": "Tl8RFyBY8cMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion I can summarize that I successfully implemented the three classifiers that utilize three different classification algorithms: random forest, feed-forward neural network, and convolutional neural network. It is obvious that the complexity of the algorithm grows with each approach. And with the grow of complexity we also witness a slight increase in accuracy of model predictions."
      ],
      "metadata": {
        "id": "zyyy0hl78L1S"
      }
    }
  ]
}